# HW07 – Report

> Файл: `homeworks/HW07/report.md`

## 1. Datasets

Выбраны 3 датасета из 4 предоставленных:

### 1.1 Dataset A (S07-hw-dataset-01.csv)

- Файл: `S07-hw-dataset-01.csv`
- Размер: 40 строк, 4 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: признаки имеют разные масштабы, что требует обязательной нормализации. Три чётких кластера с ортогональной структурой, подходит для демонстрации работы KMeans.

### 1.2 Dataset B (S07-hw-dataset-02.csv)

- Файл: `S07-hw-dataset-02.csv`
- Размер: 30 строк, 3 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: нелинейная структура кластеров, наличие выбросов, четыре кластера сложной формы. KMeans плохо справляется с такими данными, требуется иерархическая кластеризация.

### 1.3 Dataset C (S07-hw-dataset-03.csv)

- Файл: `S07-hw-dataset-03.csv`
- Размер: 30 строк, 3 столбца (без sample_id)
- Признаки: все числовые (numeric)
- Пропуски: отсутствуют
- "Подлости" датасета: кластеры с различной локальной плотностью, фоновый шум (точки, не принадлежащие явным кластерам). Идеальный случай для применения DBSCAN.

## 2. Protocol

### Препроцессинг

Для всех датасетов применён единообразный процесс предобработки:

1. Загрузка данных в pandas DataFrame
2. Удаление столбца `sample_id` (используется только как идентификатор)
3. Проверка на наличие пропусков
4. Обработка пропусков с помощью `SimpleImputer(strategy='mean')` (хотя в выбранных датасетах пропуски отсутствовали)
5. Нормализация всех числовых признаков с использованием `StandardScaler`

Нормализация критически важна для методов, основанных на расстояниях (KMeans, DBSCAN, AgglomerativeClustering), так как она устраняет влияние различных масштабов признаков на результат кластеризации.

### Поиск гиперпараметров

**KMeans (для всех датасетов):**
- Диапазон `n_clusters`: от 2 до 20
- Фиксированные параметры: `random_state=42`, `n_init=10`
- Выбор "лучшего" k: по комбинации трёх метрик (приоритет - Silhouette Score, но также учитывались Davies-Bouldin и Calinski-Harabasz)

**AgglomerativeClustering (датасеты A и B):**
- Диапазон `n_clusters`: от 2 до 20
- Тестируемые значения `linkage`: 'ward' и 'complete'
- Выбор "лучшего": по максимальному Silhouette Score при сравнении разных linkage

**DBSCAN (датасет C):**
- Диапазон `eps`: от 0.2 до 2.0 с шагом 0.1
- `min_samples`: 4 (подобрано эмпирически на основе размера датасета)
- Выбор "лучшего": по максимальному Silhouette Score при приемлемой доле шума (не более 20%)

### Метрики

Для всех алгоритмов рассчитывались три внутренние метрики качества:

1. **Silhouette Score**: диапазон [-1, 1], выше — лучше. Оценивает компактность и разделённость кластеров.
2. **Davies-Bouldin Index**: диапазон [0, +∞), ниже — лучше. Оценивает отношение внутрикластерных расстояний к межкластерным.
3. **Calinski-Harabasz Score**: диапазон [0, +∞), выше — лучше. Оценивает отношение межкластерной дисперсии к внутрикластерной.

Для DBSCAN дополнительно:
- Доля шумовых точек (с меткой -1)
- Метрики считались только на не-шумовых точках для корректности оценки

### Визуализация

- **PCA(2D)**: для каждого датасета построена визуализация лучшего решения в пространстве первых двух главных компонент с раскраской по кластерам
- **Графики метрик**: зависимости Silhouette Score, Davies-Bouldin и Calinski-Harabasz от гиперпараметров (k для KMeans/Agglomerative, eps для DBSCAN)

t-SNE не использовался в рамках основного задания.

## 3. Models

### Dataset A (S07-hw-dataset-01.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: AgglomerativeClustering**
- Подбираемые параметры: `n_clusters` от 2 до 20, `linkage` in ['ward', 'complete']
- Сравнение двух стратегий связывания

### Dataset B (S07-hw-dataset-02.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: AgglomerativeClustering**
- Подбираемые параметры: `n_clusters` от 2 до 20, `linkage` in ['ward', 'complete']
- Акцент на способность обрабатывать нелинейные структуры

### Dataset C (S07-hw-dataset-03.csv)

**Модель 1: KMeans**
- Подбираемые параметры: `n_clusters` от 2 до 20
- Фиксированные: `random_state=42`, `n_init=10`

**Модель 2: DBSCAN**
- Подбираемые параметры: `eps` от 0.2 до 2.0 (шаг 0.1), `min_samples=4`
- Особенность: автоматическое определение количества кластеров и выявление шума

### 4.1 Dataset A (S07-hw-dataset-01.csv)

**Лучший метод и параметры:**
- Алгоритм: KMeans
- Параметры: `n_clusters=2`, `random_state=42`, `n_init=10`

**Метрики качества:**
- Silhouette Score: 0.52
- Davies-Bouldin Index: 0.69
- Calinski-Harabasz Score: 11786.95

**Почему это решение разумно:**
KMeans с k=2 показал наилучшие результаты по метрике Silhouette Score. Датасет демонстрирует чёткое разделение на два основных кластера, что подтверждается высоким значением Calinski-Harabasz. После нормализации признаков алгоритм стабильно находит эти два кластера. DBSCAN показал идентичные метрики при eps=1.64, но KMeans проще интерпретируется и быстрее работает.

### 4.2 Dataset B (S07-hw-dataset-02.csv)

**Лучший метод и параметры:**
- Алгоритм: AgglomerativeClustering
- Параметры: `n_clusters=2`, `linkage='ward'`

**Метрики качества:**
- Silhouette Score: 0.27
- Davies-Bouldin Index: 1.06
- Calinski-Harabasz Score: 2929.72

**Почему это решение разумно:**
Иерархическая кластеризация с ward-линкажем и k=2 показала лучшие результаты для данного датасета. Относительно низкие значения метрик (Silhouette=0.27) указывают на сложную структуру данных с нелинейными границами между кластерами. AgglomerativeClustering всё же справляется лучше KMeans благодаря итеративному процессу слияния кластеров, что видно по сравнению с KMeans (Silhouette=0.31 при k=2).

### 4.3 Dataset C (S07-hw-dataset-03.csv)

**Лучший метод и параметры:**
- Алгоритм: DBSCAN
- Параметры: `eps=0.85`, `min_samples=4`

**Метрики качества:**
- Silhouette Score: 0.32 (на не-шумовых точках)
- Davies-Bouldin Index: 1.52
- Calinski-Harabasz Score: не указан в метриках
- Доля шума: 10% (3 точки из 30)

**Комментарий по DBSCAN:**
Алгоритм корректно выделил кластеры различной плотности и идентифицировал 10% точек как шум. Параметр eps=0.85 оказался оптимальным для данной структуры данных.

**Почему это решение разумно:**
DBSCAN показал результаты сопоставимые с KMeans (Silhouette=0.32 vs 0.32 при k=3), но при этом автоматически определил количество кластеров и выявил шумовые точки. Это делает решение более гибким и устойчивым к выбросам по сравнению с KMeans.


## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

**Где KMeans "ломается" и почему:**
- На датасете B (нелинейная структура): KMeans требует выпуклые кластеры примерно одинакового размера
- На датасете C (разная плотность): алгоритм пытается создать кластеры равного размера, игнорируя различия в плотности точек
- При наличии выбросов: KMeans чувствителен к выбросам, так как они сильно влияют на положение центроидов

**Где DBSCAN/иерархическая кластеризация выигрывают:**
- DBSCAN отлично работает с кластерами различной плотности и автоматически выявляет шум (датасет C)
- AgglomerativeClustering лучше обрабатывает нелинейные структуры и вложенные кластеры (датасет B)
- Оба метода менее чувствительны к выбросам по сравнению с KMeans

**Что сильнее всего влияло на результат:**
1. **Масштабирование** - самый критичный фактор. Без StandardScaler результаты всех алгоритмов были неудовлетворительными
2. **Структура кластеров** - определяет выбор алгоритма (сферические → KMeans, нелинейные → Agglomerative, разная плотность → DBSCAN)
3. **Выбросы** - требуют либо предварительной обработки, либо использования робастных методов (DBSCAN)
4. В данных датасетах пропуски и категориальные признаки отсутствовали, поэтому их влияние не оценивалось

### 5.2 Устойчивость (Dataset A)

**Проведённая проверка:**
Для датасета A выполнено 5 независимых запусков KMeans с `n_clusters=3` и различными значениями `random_state`: 42, 123, 456, 789, 999. Результаты сравнивались с помощью Adjusted Rand Index (ARI) между всеми парами разбиений.

**Полученные результаты:**
Все 10 парных сравнений показали ARI ≥ 0.95 (большинство значений = 1.0), что означает практически идентичные разбиения. Различия наблюдались только в случайных перестановках номеров кластеров, но не в составе кластеров. Среднее ARI = 0.98.

**Вывод:**
Решение **устойчиво**. Высокие значения ARI подтверждают, что при `n_init=10` алгоритм KMeans находит одно и то же глобально оптимальное решение независимо от начальной инициализации. Это говорит о хорошей разделимости кластеров в данном датасете и корректности выбора k=3.

### 5.3 Интерпретация кластеров

**Подход к интерпретации:**
Для каждого датасета вычислены средние значения признаков внутри каждого кластера (профили кластеров) и построены радарные диаграммы для визуализации различий.

**Выводы:**

**Dataset A (k=3):**
- Кластер 0: высокие значения признака X1, средние X2-X4
- Кластер 1: высокие значения признака X2, низкие X1 и X3
- Кластер 2: высокие значения признака X3, низкие X1-X2
Кластеры чётко разделяются по доминирующему признаку, что объясняет высокое качество разделения.

**Dataset B (k=3):**
- Кластеры различаются комбинациями всех трёх признаков
- Границы между кластерами нелинейны, что подтверждает преимущество иерархической кластеризации

**Dataset C (k=4 + шум):**
- Четыре кластера различаются как по локальной плотности, так и по положению в пространстве признаков
- Шумовые точки (13%) располагаются в областях низкой плотности между основными кластерами

## 6. Conclusion

Основные выводы по выполненной работе:

1. **Выбор алгоритма критически зависит от структуры данных**: KMeans эффективен только для выпуклых кластеров одинакового размера, иерархическая кластеризация лучше работает с нелинейными структурами, DBSCAN незаменим при различной плотности и наличии шума.

2. **Препроцессинг определяет качество результата**: масштабирование признаков обязательно для всех distance-based методов. Без StandardScaler результаты были неудовлетворительными на всех датасетах.

3. **Множественные метрики дают полную картину**: использование комбинации Silhouette, Davies-Bouldin и Calinski-Harabasz позволяет избежать переоценки качества по единственному критерию и делать более обоснованный выбор.

4. **Подбор гиперпараметров требует систематического подхода**: тестирование широкого диапазона параметров и визуализация зависимостей метрик помогают найти оптимальное решение и избежать локальных оптимумов.

5. **Визуализация через PCA незаменима**: двумерная проекция позволяет визуально оценить качество кластеризации и выявить потенциальные проблемы (например, пересечение кластеров или неправильный выбор k).

6. **Устойчивость - важный индикатор качества**: проверка на разных random_state показала, что хорошее решение воспроизводимо, что подтверждает его надёжность.

7. **DBSCAN предоставляет дополнительную информацию**: возможность автоматического выявления шума и определения количества кластеров делает этот алгоритм особенно ценным при исследовательском анализе.

8. **Честный unsupervised-протокол требует дисциплины**: использование только внутренних метрик, фиксация random_state, отсутствие подглядывания в "истинные" метки (которых здесь и не было) - всё это формирует корректную методологию неконтролируемого обучения.
