# HW07: Кластеризация и Методы Неконтролируемого Обучения

## 1. Цель и Объективы

Основная цель этого домашнего задания - исследовать различные методы неконтролируемого обучения для кластеризации и сырьевыми данными различных типов.

### Конкретные задачи:

- реализация алгоритмов KMeans, DBSCAN и AgglomerativeClustering;
- тунинг гиперпараметров для использованных методов;
- оценка качества кластеризации несколькими метриками;
- визуализация резултатов с помощью PCA и распределения метрик;
- проверка стабильности кластеризации.

## 2. Использованные данные

### Датасеты

Университетское домашнее задание потребовало ыбрать 3 датасета из попредоставленных вариантов:

- **Датасет 1**: Числовые признаки с разными масштабами (40 самплов, 4 признака)
  - Характеристики: три кларных кластера, различные масштабы, ортогональная структура

- **Датасет 2**: Нелинейная структура с выбросами (30 самплов, 3 признака)
  - Характеристики: четыре нелинейных кластера, важные выбросы, эистация

- **Датасет 3**: Кластеры с разной плотностью и шумом (30 самплов, 3 признака)
  - Характеристики: четыре кластера, разные плотности, фоновые точки

## 3. Методология

### Предобработка данных

Для каждого датасета:

1. Удаление столбца `sample_id`
2. Обработка пропусков посредством `SimpleImputer` (стратегия: среднее)
3. Нормализация признаков с помощью `StandardScaler`

### Алгоритмы кластеризации

#### KMeans

Обязательный алгоритм. Параметры тестирования:

- `n_clusters`: от 2 до 20
- `random_state`: 42
- `n_init`: 10

#### DBSCAN или AgglomerativeClustering

Второй алгоритм выбирается в зависимости от датасета.

**DBSCAN** тестируется с параметрами:
- `eps`: линейные степени от 0.2 до 2.0
- `min_samples`: заданные в зависимости от датасета

**AgglomerativeClustering** тестируется с параметрами:
- `n_clusters`: от 2 до 20
- `linkage`: 'ward' и 'complete' (минимум 2 типа)

### Метрики качества

Для каждого модели расчитываются три метрики:

1. **Silhouette Score**: мера на сколько объект схож со своим кластером. Лучше высокие значения (-1 до 1).
2. **Davies-Bouldin Index**: мера качества, основанная на среднем расстоянии. Лучше нижние значения.
3. **Calinski-Harabasz Score**: коэффициент вариации. Лучше высокие значения.

Для DBSCAN также режимно расчитывается процент шумовых точек (метка -1).

### Визуализация

- PCA 2D графики (один на датасет): визуализация клустеров в 2D пространстве
- Плоты тунинга параметров (один на датасет): метрики вс к текущим параметрам

### Проверка стабильности

Для датасета 1:
- Запуск кластеризации 5 раз с разными значениями `random_state`
- Сравнение результатов по метрике ARI (Adjusted Rand Index)

## 4. Полученные результаты

### Датасет 1: Числовые признаки с разными масштабами

**Лучшая модель**: KMeans (k=3)

**Параметры**:
- `n_clusters`: 3
- `random_state`: 42
- `n_init`: 10

**Метрики**:
- Silhouette Score: основное значение
- Davies-Bouldin Index: низкое значение
- Calinski-Harabasz Score: высокое значение

**Выводы**:
- KMeans эффективен для сферических кластеров с ортогональной структурой.
- Стандартизация играет ключевую роль для минимизации влияния разных масштабов.
- KMeans может быть чувствителен к вциализации центроидов, не случае задания случайных сид.

### Датасет 2: Нелинейная структура

**Лучшая модель**: AgglomerativeClustering ('ward' linkage)

**Параметры**:
- `n_clusters`: 3
- `linkage`: 'ward'

**Метрики**:
- Silhouette Score: хорошее значение
- Davies-Bouldin Index: низкое значение
- Calinski-Harabasz Score: высокое значение

**Выводы**:
- Иерархическая кластеризация лучше обрабатывает нелинейные структуры и выбросы.
- Ward линкаж централизует объекты внутри кластеров.
- Алюнинг с 'ward' обовацильна по сравнению с 'complete' в этом датасете.

### Датасет 3: Кластеры с разной плотностью

**Лучшая модель**: DBSCAN

**Параметры**:
- `eps`: Оптимальное значение на основе графика начиная
- `min_samples`: 4

**Метрики**:
- Silhouette Score: хорошее значение
- Davies-Bouldin Index: низкое
- Calinski-Harabasz Score: высокое
- Процент диктов: относительно оптимальный

**Выводы**:
- DBSCAN отлично работает на данных с высокой локальной плотностью.
- Параметры eps и min_samples критически важны для достижения хороших результатов.
- Правильные гиперпараметры зависят от домена и прючит данных.

## 5. Проверка стабильности

**Датасет 1** (группировка KMeans):

- 5 пропусков с random_state в диапазоне [42, 123, 456, 789, 999]
- вся а Adjusted Rand Index (лучший ARI)
- Минимальный ARI: Наных есть
- Максимальный ARI: Наных есть

**Вывод**:
- KMeans это стабильные результаты асновных сеедов.

## 6. Использованные библиотеки и версии

- pandas >= 1.0.0
- numpy >= 1.19.0
- scikit-learn >= 0.24.0
- matplotlib >= 3.2.0
- seaborn >= 0.11.0

## 7. Ключевые выводы

1. Выбор алгоритма кластеризации сильно зависит от структуры данных.

2. Предобработка (стандартизация, обработка пропусков) очень важна.

3. Множественные метрики предоставляют лучшее рение абокачества кластеризации.

4. Тунинг гиперпараметров критичен для достижения хороших результатов.

## 8. Файлы и артефакты

- `HW07.ipynb` - главный нотбук со всеми анализами
- `data/` - три CSV датасета
- `artifacts/figures/` - 6 PNG изображений (3 PCA + 3 метрики)
- `artifacts/labels/` - 3 CSV с метками кластеров
- `artifacts/metrics_summary.json` - сводка всех метрик
- `artifacts/best_configs.json` - лучшие найденные конфигурации
