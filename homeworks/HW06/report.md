# HW06 – Report

> Файл: `homeworks/HW06/report.md`  
> Студент: Фех Алексей Александрович

## 1. Dataset

- Какой датасет выбран: `S06-hw-dataset-02.csv`
- Размер: (18000 строк, 39 столбцов)
- Целевая переменная: `target` (0: 73.7%, 1: 26.3%)
- Признаки: 37 числовых признаков (f01-f35, x_int_1, x_int_2), все вещественные значения

Выбран dataset-02, потому что он сложный с нелинейными взаимодействиями - на нем хорошо видно преимущество ансамблей над простыми моделями.

## 2. Protocol

- Разбиение: train/test (75%/25%, `random_state=42`)
- Стратификация: используется `stratify=y` для сохранения пропорций классов
- Подбор: CV на train (5 фолдов, оптимизация по ROC-AUC)
- Метрики: accuracy, F1, ROC-AUC

Эти метрики уместны, потому что:
- Accuracy - общая точность классификации
- F1 - баланс precision/recall, важен при дисбалансе классов
- ROC-AUC - способность модели разделять классы, не зависит от порога

## 3. Models

Сравнил следующие модели:

1. **DummyClassifier** (baseline) - стратегия `most_frequent`
2. **LogisticRegression** (baseline) - с StandardScaler, max_iter=1000
3. **DecisionTreeClassifier** - контроль сложности: max_depth=10, min_samples_leaf=20
4. **RandomForestClassifier** - n_estimators=100, max_depth=15, min_samples_leaf=10, max_features='sqrt'
5. **GradientBoostingClassifier** - n_estimators=100, max_depth=5, learning_rate=0.1

Для деревьев подбирал max_depth и min_samples_leaf чтобы избежать переобучения.

## 4. Results

| Модель | Accuracy | F1 | ROC-AUC |
|--------|----------|-------|---------|
| DummyClassifier | 0.7373 | 0.0000 | - |
| LogisticRegression | 0.8162 | 0.5717 | 0.8009 |
| DecisionTree | 0.8367 | 0.6286 | 0.8129 |
| RandomForest | 0.8513 | 0.6789 | 0.8645 |
| **GradientBoosting** | **0.8578** | **0.6921** | **0.8712** |

**Победитель**: GradientBoosting с ROC-AUC = 0.8712

Градиентный бустинг показал лучшие результаты благодаря последовательному улучшению модели и способности находить сложные нелинейные зависимости в данных.

## 5. Analysis

**Устойчивость**: Проверил GradientBoosting с разными random_state (42, 123, 456, 789, 999):
- ROC-AUC варьируется в пределах 0.868-0.874
- Модель стабильна, разброс небольшой

**Ошибки**: Confusion matrix для GradientBoosting:
- True Negatives: 3156 (правильно предсказан класс 0)
- False Positives: 162 (ошибочно предсказан класс 1)
- False Negatives: 478 (пропущен класс 1)
- True Positives: 704 (правильно предсказан класс 1)

Модель лучше распознает мажоритарный класс (0), но достаточно хорошо находит и класс 1.

**Интерпретация**: Top-10 признаков по permutation importance:
1. f04, f07, f15 - сильно влияют на предсказание
2. x_int_1, x_int_2 - созданные взаимодействия признаков важны
3. f20, f25, f30 - также вносят вклад

Это соответствует описанию датасета как содержащего нелинейные взаимодействия.

## 6. Conclusion

- Деревья решений легко переобучаются без контроля сложности (max_depth, min_samples_leaf)
- Random Forest снижает variance за счет усреднения множества деревьев
- Boosting эффективнее bagging на сложных данных благодаря последовательной коррекции ошибок
- Честный ML-протокол (фиксированный train/test, CV только на train) критически важен для объективной оценки
- Одной accuracy недостаточно при дисбалансе классов - нужны F1 и ROC-AUC
